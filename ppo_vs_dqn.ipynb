{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from battle_environment import BattleEnvironment\n",
    "import os\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 100):\n",
    "    if not os.path.exists(f'models/models{i}'):\n",
    "        FOLDER =  f'models/models{i}'\n",
    "        LOG_DIR = f'{FOLDER}/logs'\n",
    "        os.makedirs(FOLDER)\n",
    "        os.makedirs(LOG_DIR)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winrate Graph\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='# of games played', ylabel='Percentage of agent wins (%)')\n",
    "ax.grid()\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, env, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.num_wins = 0\n",
    "        self.total_games = 0\n",
    "        self.games = 0\n",
    "        self.env = env\n",
    "        self.total_wins = 0\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, f'{self.n_calls}')\n",
    "            self.model.save(model_path)\n",
    "        if self.n_calls % 10000 == 0 and self.env.team['red']['wins'] > 0 and self.n_calls > 0:\n",
    "            self.num_wins = self.env.team['red']['wins'] - self.total_wins\n",
    "            self.games = self.env.total_games - self.total_games\n",
    "            self.total_wins = self.env.team['red']['wins']\n",
    "            self.total_games = self.env.total_games\n",
    "            print(f\"\\n\\n\\n\\n--------------------\\ntotal games:{self.total_games}\\ntimesteps:{self.n_calls}\\nwin percentage:{round(self.num_wins/self.games * 100, 2)}\\n--------------------\\n\\n\\n\\n\")\n",
    "            x.append(self.env.total_games)\n",
    "            y.append(self.num_wins/self.games * 100)\n",
    "        return True\n",
    "\n",
    "save_freq = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = {\n",
    "    'hit_base_reward': 10,\n",
    "    'hit_plane_reward': 1,\n",
    "    'miss_punishment': 0,\n",
    "    'too_long_punishment': 0,\n",
    "    'lose_punishment': -3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 10000000\n",
    "saved_timesteps = timesteps // save_freq * save_freq\n",
    "file = open(f\"{FOLDER}/results.txt\", 'a')\n",
    "print(f\"Timesteps:{saved_timesteps}\\nConfig:{pprint.pformat(cf)}\", file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = f'{FOLDER}/train_PPO'\n",
    "\n",
    "# Create the environment and model\n",
    "env = BattleEnvironment(show=False, hit_base_reward=cf['hit_base_reward'], hit_plane_reward=cf['hit_plane_reward'], miss_punishment=cf['miss_punishment'], \n",
    "    too_long_punishment=cf['too_long_punishment'], lose_punishment=cf['lose_punishment'])\n",
    "callback = TrainAndLoggingCallback(check_freq=save_freq, save_path=CHECKPOINT_DIR, env=env)\n",
    "model = PPO('MlpPolicy', env, tensorboard_log=LOG_DIR, verbose=1)\n",
    "\n",
    "# Train the model and save graph\n",
    "model.learn(total_timesteps=timesteps, callback=callback)\n",
    "model.save(f\"{FOLDER}/final_model_PPO\")\n",
    "del model\n",
    "\n",
    "ax.plot(x, y)\n",
    "x.clear()\n",
    "y.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = f'{FOLDER}/train_PPO'\n",
    "\n",
    "# Create the environment and model\n",
    "env = BattleEnvironment(show=False, hit_base_reward=cf['hit_base_reward'], hit_plane_reward=cf['hit_plane_reward'], miss_punishment=cf['miss_punishment'], \n",
    "    too_long_punishment=cf['too_long_punishment'], lose_punishment=cf['lose_punishment'])\n",
    "callback = TrainAndLoggingCallback(check_freq=save_freq, save_path=CHECKPOINT_DIR, env=env)\n",
    "model = DQN('MlpPolicy', env, tensorboard_log=LOG_DIR, verbose=1)\n",
    "\n",
    "# Train the model and save graph\n",
    "model.learn(total_timesteps=timesteps, callback=callback)\n",
    "model.save(f\"{FOLDER}/final_model_DQN\")\n",
    "del model\n",
    "\n",
    "ax.plot(callback.x, callback.y)\n",
    "fig.savefig(f\"{FOLDER}/percent_win.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained agent and evaluate 1000 games\n",
    "eval_env = BattleEnvironment(show=False, hit_base_reward=cf['hit_base_reward'], hit_plane_reward=cf['hit_plane_reward'], miss_punishment=cf['miss_punishment'], \n",
    "    too_long_punishment=cf['too_long_punishment'], lose_punishment=cf['lose_punishment'])\n",
    "model = PPO.load(f\"{FOLDER}/final_model_PPO\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=1000, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "print(eval_env.wins())\n",
    "print(f\"\\n---EVALUATION PPO---\\n{eval_env.wins()}\\n\", file=file)\n",
    "\n",
    "# Evaluate with visuals (10 games)\n",
    "eval_env = BattleEnvironment(show=True, hit_base_reward=cf['hit_base_reward'], hit_plane_reward=cf['hit_plane_reward'], miss_punishment=cf['miss_punishment'], \n",
    "    too_long_punishment=cf['too_long_punishment'], lose_punishment=cf['lose_punishment'], fps=30)\n",
    "model = PPO.load(f\"{FOLDER}/final_model\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "print(eval_env.wins())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained agent and evaluate 1000 games\n",
    "eval_env = BattleEnvironment(show=False, hit_base_reward=cf['hit_base_reward'], hit_plane_reward=cf['hit_plane_reward'], miss_punishment=cf['miss_punishment'], \n",
    "    too_long_punishment=cf['too_long_punishment'], lose_punishment=cf['lose_punishment'])\n",
    "model = DQN.load(f\"{FOLDER}/final_model_DQN\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=1000, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "print(eval_env.wins())\n",
    "print(f\"\\n---EVALUATION DQN---\\n{eval_env.wins()}\\n\", file=file)\n",
    "\n",
    "# Evaluate with visuals (10 games)\n",
    "eval_env = BattleEnvironment(show=True, hit_base_reward=cf['hit_base_reward'], hit_plane_reward=cf['hit_plane_reward'], miss_punishment=cf['miss_punishment'], \n",
    "    too_long_punishment=cf['too_long_punishment'], lose_punishment=cf['lose_punishment'], fps=30)\n",
    "model = DQN.load(f\"{FOLDER}/final_model\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "print(eval_env.wins())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f9c2e007771efceb85382aa011b8a6a8e29c72f8a97441d236f12248428b2c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
